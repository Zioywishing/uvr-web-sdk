# UAR Web SDK 技术实现原理

本项目是一个基于 Web 技术的音频处理 SDK，主要用于实现高质量的音频分离或增强（如 UVR - Ultimate Vocal Remover 的 Web 版本）。它利用了 WebAssembly (WASM)、WebGPU 和 Web Workers 等现代 Web 技术，在浏览器端实现高性能的数字信号处理 (DSP) 和深度学习模型推理。

## 1. 整体架构设计

项目采用 Monorepo 结构，通过 Bun 进行管理。核心模块分为以下几个部分：

- **`@uvr-web-sdk/core`**: SDK 的核心逻辑，负责音频解码、重采样、任务调度以及流水线管理。
- **`@uvr-web-sdk/fft`**: 基于 WASM 的高性能快速傅里叶变换 (FFT) 库，封装了 `kissfft`。
- **`@uvr-web-sdk/ort-runtime-webgpu`**: 封装了 ONNX Runtime，支持 WebGPU 加速的深度学习推理。
- **`Web Workers`**: 将计算密集型任务（FFT、模型推理、IFFT）移出主线程，确保 UI 响应。

---

## 2. 核心实现步骤与原理

### 第一步：音频预处理 (Audio Pre-processing)

**做了什么：**
1.  使用 `AudioContext.decodeAudioData` 将输入的音频 ArrayBuffer 解码为原始的 PCM 脉冲编码调制数据。
2.  检查采样率。如果不是 44,100Hz，则进行重采样。

**为什么这样做：**
- **解码**：深度学习模型通常处理的是原始波形数据或其频域表示，而不是压缩格式（如 MP3/AAC）。
- **重采样**：模型是在特定采样率（通常是 44.1kHz）下训练的。输入采样率不匹配会导致频率偏移，严重影响效果。

**数学原理：**
- **采样定理 (Nyquist-Shannon)**：确保信号可以被无失真地还原。
- **线性插值/重采样**：在时间轴上重新采样点，通过数学插值算法计算新采样点的值。

### 第二步：短时傅里叶变换 (STFT)

**做了什么：**
在 `worker-fft.ts` 中，将长音频切分为重叠的段（Segments），对每一段应用 **Hann 窗 (Hann Window)**，然后计算 **实数快速傅里叶变换 (RFFT)**。

**核心参数 (Core Parameters) 详解：**
在 `index.ts` 中定义的几个关键常量决定了处理的精度和范围：
- **`nfft = 6144` (Window Size)**：FFT 窗口大小。较大的窗口提供更高的频率分辨率，但时间分辨率会降低。
- **`hop = 1024` (Hop Size)**：相邻 STFT 帧之间的步长。`nfft / hop = 6`，表示帧之间有约 83.3% 的重叠，这确保了频域特征的连续性。
- **`dimF = 3072`**：频率桶数量。$dimF = nfft / 2$。
- **`dimT = 256`**：一个处理单元（Chunk）内的帧数。
- **`chunkSize = 261120`**：计算公式为 `hop * (dimT - 1)`。它代表了从第一帧起点到最后一帧起点的距离。
- **`segExt = 267264`**：实际处理长度，即 `chunkSize + nfft`。这是模型一次推理所涵盖的完整 PCM 采样点总数。
- **`segStep = 254976`**：计算公式为 `chunkSize - nfft`。这是流水线前进的步长。
    - **重叠机制**：由于 `segStep < segExt`，相邻 Chunk 之间存在 `2 * nfft` 的重叠区域。这部分重叠用于执行跨 Chunk 的平滑淡入淡出（Cross-fade），消除块与块之间的拼接痕迹。

**为什么这样做：**
- **分段**：音频信号是非平稳的，但在短时间内可以看作是平稳的。
- **加窗**：直接切分会导致频谱泄漏（Spectral Leakage）。Hann 窗可以将信号边缘平滑到零，减少断裂。
- **重叠 (Overlap)**：为了在还原时通过 Overlap-Add 消除分段带来的边缘效应。

**数学原理与细节：**
- **输入输出映射**：
    - **输入**：每一帧包含 $N$ 个时域采样点（本项目中 $N=6144$）。
    - **输出**：经过 RFFT 后，得到 $\frac{N}{2} + 1$ 个复数点（Bin）。每个点由一个实部 ($Re$) 和一个虚部 ($Im$) 组成。
- **实部与虚部的物理意义**：
    - 每一个复数 $X[k] = Re + j \cdot Im$ 代表了频率为 $f_k = \frac{k \cdot f_s}{N}$ 的正弦波分量。
    - **幅度 (Magnitude)**：$|X[k]| = \sqrt{Re^2 + Im^2}$。它反映了信号在该频率上的能量大小（即“音量”）。
    - **相位 (Phase)**：$\theta = \operatorname{atan2}(Im, Re)$。它反映了该频率分量相对于时间零点的起始偏移位置。
- **模型输入构造**：
    - 为了让 AI 模型能够处理这些信息，我们将实部和虚部分别作为独立的通道。
    - 对于双声道音频，输入张量包含 4 个通道：`[L_Real, L_Imag, R_Real, R_Imag]`。
- **公式**：
    - **FFT 公式**：$X[k] = \sum_{n=0}^{N-1} x[n] e^{-j\frac{2\pi}{N}nk}$
    - **Hann 窗公式**：$w[n] = 0.5 \left(1 - \cos\left(\frac{2\pi n}{N-1}\right)\right)$
    - **STFT**：通过滑动窗口在时域和频域之间建立映射，生成声谱图（Spectrogram）。

### 第三步：深度学习模型推理 (Neural Inference)

**做了什么：**
在 `worker-ort.ts` 中，将 STFT 产生的复数声谱图（实部和虚部）作为张量输入到 ONNX 模型中。

**张量形态 (Tensor Shape) 详解：**
模型接收的输入张量维度通常为 `[1, 4, 3072, 256]`，其具体含义如下：
- **`1` (Batch Size)**：批处理大小。在 Web 端通常一次只处理一个音频分段（Chunk）。
- **`4` (Channels)**：通道数。对应双声道音频的复数表示：`[左声道实部, 左声道虚部, 右声道实部, 右声道虚部]`。
- **`3072` (Frequency/dimF)**：频率轴分辨率。由 `nfft / 2` 决定（$6144 / 2 = 3072$）。它代表了从 0Hz 到 Nyquist 频率（采样率的一半，约 22050Hz）被切分成了 3072 个频率区间。
- **`256` (Time/dimT)**：时间轴长度。一个 Chunk 内包含的 STFT 帧数。配合 `hop=1024` 的步长，这 256 帧覆盖了约 5.9 秒的音频（$256 \times 1024 / 44100 \approx 5.94s$）。

**为什么这样做：**
- **维度对齐**：CNN 模型需要固定的输入尺寸来执行卷积核运算。
- **WebGPU**：利用显卡进行并行计算，速度比 CPU (WASM) 快数倍。
- **ONNX Runtime**：提供通用的模型运行环境，支持各种复杂的神经网络层。

**数学原理：**
- **卷积神经网络 (CNN)**：在声谱图上进行特征提取，识别并分离不同的音频成分（如人声与背景音乐）。
- **张量运算**：模型推理本质上是大量的矩阵乘法和激活函数运算。

### 第四步：逆短时傅里叶变换 (ISTFT) 与 加权重叠相加 (WOLA)

**做了什么：**
在 `worker-ifft.ts` 中，对模型输出的频域数据进行 **逆快速傅里叶变换 (IFFT)**，再次应用 Hann 窗，并使用 **加权重叠相加 (WOLA)** 技术重构时域信号。

**为什么这样做：**
- **IFFT**：将处理后的频域信息还原回时间信号。
- **WOLA**：通过两次加窗（FFT 前一次，IFFT 后一次）并除以窗口平方和的归一化因子，可以实现“完美重构”（Perfect Reconstruction），消除相位不连续导致的“咔哒”声。

**数学原理：**
- **IFFT 公式**：$x[n] = \frac{1}{N} \sum_{k=0}^{N-1} X[k] e^{j\frac{2\pi}{N}nk}$
- **OLA 归一化**：$x_{final}[n] = \frac{\sum segments}{\sum w^2[n]}$。由于使用了 Hann 窗且重叠率合适（如 75% 或更高），$\sum w^2$ 是一个常数，确保了信号幅度的平稳。

### 第五步：并行流水线调度 (Pipeline Scheduling)

**做了什么：**
`core` 模块实现了 `runPipelineStream`。它将任务拆分为多个阶段，允许 FFT Worker 正在处理第 N+1 段时，ORT Worker 正在处理第 N 段，IFFT Worker 正在处理第 N-1 段。

**为什么这样做：**
- **最大化吞吐量**：类似于工业流水线，提高 CPU/GPU 的利用率，显著缩短总处理时间。
- **流式输出**：通过 `ReadableStream` 实时向前端返回处理结果，用户无需等待整个音频处理完成即可开始试听。

---

## 3. 关键性能优化

1.  **SharedArrayBuffer / Transferables**：在 Worker 之间传递数据时，使用 Transferable Objects（如 `[buffer]`）转移内存所有权，避免了昂贵的内存拷贝。
2.  **WASM 优化**：FFT 运算由编译为 WASM 的 C 语言代码执行，接近原生性能。
3.  **重采样热身**：预先初始化 `AudioContext`，减少首次点击时的延迟。
4.  **内存管理**：使用 `normBuffer` 和输入/输出缓冲区循环利用内存，减少垃圾回收 (GC) 压力。
